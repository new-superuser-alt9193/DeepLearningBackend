{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get K value for K Means\n",
    "def getK(x):\n",
    "    scaled_x = x\n",
    "    distortions = {}\n",
    "    i = 1\n",
    "    while True:\n",
    "        #fit k means clustering according to i\n",
    "        km = KMeans(\n",
    "            n_clusters= i, init='random',\n",
    "            n_init=10, max_iter=300, \n",
    "            tol=1e-04, random_state=0\n",
    "        ).fit(scaled_x)\n",
    "        #get distortion of actual k, which is the sum distance between clusters and their centroid\n",
    "        current_distortion = sum(np.min(cdist(scaled_x, km.cluster_centers_,'euclidean'), axis=1)) / scaled_x.shape[0] \n",
    "        distortions[i] = current_distortion\n",
    "        #getting 3 iterations\n",
    "        if i >= 3:\n",
    "            #get slope between i -1 and i - 2, i and i - 1\n",
    "            m1 = distortions[i - 2] - distortions[i - 1]\n",
    "            m2 = distortions[i - 1] - distortions[i]\n",
    "            #get the differential between slopes and addition\n",
    "            m_dif = m1 - m2\n",
    "            m_sum = m1 + m2\n",
    "            #get the percentage representation of differential, since 100% equals to the sum of slope values\n",
    "            dif_percentage = (m_dif * 100) / m_sum\n",
    "            #if this percentage is less than 25%, it means that  distortion will have a linear behaviour as more k iterations\n",
    "            #so we can say that a correct k value for optimal clustering is i - 2.\n",
    "            if dif_percentage < 25.0:\n",
    "                break\n",
    "        i += 1\n",
    "    return i - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeClusters(n):\n",
    "    km = KMeans(\n",
    "        n_clusters= n, init='random',\n",
    "        n_init=10, max_iter=300, \n",
    "        tol=1e-04, random_state=0\n",
    "    )\n",
    "    y_km = km.fit_predict(x)\n",
    "    cluster_labels = km.labels_\n",
    "    return km, y_km, cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clusters(file_path, file_name):\n",
    "    df = dd.read_csv(file_path + \"/\" + file_name + \".csv\")\n",
    "    df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "    x = np.array(df.drop(columns=['TARGET']))\n",
    "\n",
    "    distortions = []\n",
    "    inertias = []\n",
    "    mapping1 = {}\n",
    "    mapping2 = {}\n",
    "    K = range(1, 10)\n",
    "    scaled_x = x \n",
    "    for k in K:\n",
    "        # Building and fitting the model\n",
    "        kmeanModel = KMeans(n_clusters=k).fit(scaled_x)\n",
    "    \n",
    "        distortions.append(sum(np.min(cdist(scaled_x, kmeanModel.cluster_centers_,\n",
    "                                            'euclidean'), axis=1)) / scaled_x.shape[0])\n",
    "        inertias.append(kmeanModel.inertia_)\n",
    "    \n",
    "        mapping1[k] = sum(np.min(cdist(scaled_x, kmeanModel.cluster_centers_,\n",
    "                                    'euclidean'), axis=1)) / scaled_x.shape[0]\n",
    "        mapping2[k] = kmeanModel.inertia_\n",
    "\n",
    "\n",
    "    n = getK(x)\n",
    "    km, y_km, km_labels = makeClusters(n)\n",
    "\n",
    "    clusters = pd.DataFrame(data = {'cluster': y_km})\n",
    "\n",
    "    # Une la clasificacion con los datos del dataset\n",
    "    df_clusters = dd.merge(clusters, df.drop(columns=['TARGET']), left_index=True, right_index=True)\n",
    "\n",
    "    df_clusters = dd.merge(df_clusters, df[['TARGET']], left_index=True, right_index=True)\n",
    "\n",
    "    # sort the dataframe\n",
    "    df_clusters = df_clusters.sort_values(by=['cluster'])\n",
    "\n",
    "    info = []\n",
    "    amount = 0\n",
    "    for i in range (n):\n",
    "        df_to_csv = df_clusters[df_clusters['cluster'] == i]\n",
    "        clusters_amount = df_to_csv.shape[0].compute()\n",
    "        amount += clusters_amount \n",
    "        info.append({\"name\": \"cluster_\" + str(i + 1), \"percentage\": clusters_amount})\n",
    "        df_to_csv.to_csv(file_path + \"/cluster/cluster_\" + str(i) + \"/cluster.csv\", single_file=True)\n",
    "    for i in info:\n",
    "        i[\"percentage\"] = i[\"percentage\"] / amount\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'cluster_1', 'percentage': 0.9922587129012082},\n",
       " {'name': 'cluster_2', 'percentage': 0.007741287098791895}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_clusters(\"/home/alt9193/Documents/IA/DeepLearningBackend/examples/\", \"telecom_pca\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
